{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d82e63",
   "metadata": {},
   "source": [
    "Notebook to do a MNIST-neural network from scratch based on the data provide by YannLecun http://yann.lecun.com/exdb/mnist/ (see also the paper \"Deep learning\" by Yann LeCun1, Yoshua Bengio & Geoffrey Hinton in NATURE, VOL 521, 28 MAY 201) and the tutorial https://github.com/numpy/numpy-tutorials/blob/main/content/tutorial-deep-learning-on-mnist.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files={\"test_images\":\"t10k-images-idx3-ubyte.gz\",\n",
    "        \"test_labels\":\"t10k-labels-idx1-ubyte.gz\",\n",
    "        \"train_images\":\"train-images-idx3-ubyte.gz\",\n",
    "     \"train_labels\":\"train-labels-idx1-ubyte.gz\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = {}\n",
    "# Images\n",
    "for key in (\"train_images\", \"test_images\"):\n",
    "    with gzip.open(os.path.join(\"\", files[key]), \"rb\") as mnist_file:\n",
    "        mnist_dataset[key] = np.frombuffer(\n",
    "            mnist_file.read(), np.uint8, offset=16\n",
    "        ).reshape(-1, 28 * 28)\n",
    "# Labels\n",
    "for key in (\"train_labels\", \"test_labels\"):\n",
    "    with gzip.open(os.path.join(\"\", files[key]), \"rb\") as mnist_file:\n",
    "        mnist_dataset[key] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the data into training and test set\n",
    "x_train, y_train, x_test, y_test = (\n",
    "    mnist_dataset[\"train_images\"],\n",
    "    mnist_dataset[\"train_labels\"],\n",
    "    mnist_dataset[\"test_images\"],\n",
    "    mnist_dataset[\"test_labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c429ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train.shape= \",x_train.shape)\n",
    "print(\"y_train.shape= \",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Take the 60,000th image (indexed at 59,999) from the training set,\n",
    "# reshape from (784, ) to (28, 28) to have a valid shape for displaying purposes.\n",
    "mnist_image = x_train[59999, :].reshape(28, 28)\n",
    "# Set the color mapping to grayscale to have a black background.\n",
    "plt.imshow(mnist_image, cmap=\"gray\")\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data type of training images: {}\".format(x_train.dtype))\n",
    "print(\"The data type of test images: {}\".format(x_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample, test_sample = 1000, 1000\n",
    "training_images = x_train[0:training_sample] / 255\n",
    "test_images = x_test[0:test_sample] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42692ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data type of training images: {}\".format(training_images.dtype))\n",
    "print(\"The data type of test images: {}\".format(test_images.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, dimension=10):\n",
    "    # Define a one-hot variable for an all-zero vector\n",
    "    # with 10 dimensions (number labels from 0 to 9).\n",
    "    one_hot_labels = labels[..., None] == np.arange(dimension)[None]\n",
    "    # Return one-hot encoded labels.\n",
    "    return one_hot_labels.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = one_hot_encoding(y_train[:training_sample])\n",
    "test_labels = one_hot_encoding(y_test[:test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data type of training labels: {}\".format(training_labels.dtype))\n",
    "print(\"The data type of test labels: {}\".format(test_labels.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in range(10)])\n",
    "print(\"---------------- label ----------------\")\n",
    "print(training_labels[0])\n",
    "print(training_labels[1])\n",
    "print(training_labels[2])\n",
    "print(\"---------------- original ----------------\")\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf2bba",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f7652",
   "metadata": {},
   "source": [
    "Architecture of the ANN:  \n",
    "one input layer of 784 nodes \n",
    "Two hidden layer:  \n",
    "   - one of 28x28 =392 nodes\n",
    "   - one of 98\n",
    "   (or 196 if trying with a third - one of 28)\n",
    "   \n",
    "one output layer of 10 nodes  (one for each number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc9dd2",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b769593",
   "metadata": {},
   "source": [
    "As the network is a multilayer perceptron for a multiclass classification problem, we use a leaky ReLU activation function in the first hidden layer then a swish activation function in the second hidden layer and a softmax activation function in the final layer. \n",
    "\n",
    "We choose sofmax in the last layer because it's the layer with the less node hence it's not very important if this function is a little heavy computing due to the exponential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81aa89",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8961a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = lambda x : np.maximum(0.1*x,x)\n",
    "relu = lambda x :  np.maximum(0.,x)\n",
    "#sigmoid = lambda x : 1./(1.+np.exp(-x+max(x))) \n",
    "def sigmoid( x ):\n",
    "    # Prevent overflow.\n",
    "    x = np.clip( x, -500, 500 )\n",
    "    return 1.0/( 1.0 + np.exp( -x ))\n",
    "\n",
    "swish = lambda x : (x-max(x))*sigmoid(x) # in case of need of omprovement, could use x*sigma_swish(beta*x) \n",
    "softmax = lambda x :np.exp(x-max(x))/np.sum(np.exp(x-max(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4b890",
   "metadata": {},
   "source": [
    "## Backward activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a4678",
   "metadata": {},
   "source": [
    "To give our network the ability to learn we define optimizers.\n",
    "\n",
    "First we define a loss function. We use softmax as final activation function hence we use the categorical cross entropy \n",
    "$ CSE = \\sum_i^n (-y_i log(\\hat y_i) + (1-y_i)(-log(1 -\\hat y_i)))$ where $ \\hat y_i $ is the predicted value and n the number of classes and $n$ is the number of class. (see https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab store the list of couple [y_true, y_prev]\n",
    "#cse  = lambda true,prev : sum([true[i][0]*(-np.log(prev[i][0]))+(1-true[i][0])*(-np.log(1 - prev[i][0])) for i in range(len(true))])   \n",
    "def cse(X,Y):\n",
    "    epsilon=1e-7\n",
    "    loss=-np.sum(X *np.log(Y+epsilon))\n",
    "    loss-=np.sum((1.0-X) * (np.log(1.0-Y+epsilon)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd254e",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a50969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_derivative(y_true, y_pred):\n",
    "    num_samples = y_true.shape[0]\n",
    "    # Avoid division by zero by clipping the predicted values\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    # Compute the derivative of categorical cross entropy loss\n",
    "    gradient = (-(y_true / y_pred_clipped) + (1-y_true) / (1-y_pred_clipped))/num_samples\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_derivative(y_true, y_pred):\n",
    "    c = np.zeros((y_true.shape[0],1))\n",
    "    for i in range(y_true.shape[0]):\n",
    "        c[i][0]=-(y_true[i][0] / (y_pred[i][0] +1e-7) + (1-y_true[i][0]) / (1-y_pred[i][0]+1e-7))\n",
    "    return c         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9127b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives of the activation function\n",
    "backward_leaky_relu = lambda x: np.where(x > 0., 1., 0.1)\n",
    "backward_relu = lambda x: (x > 0.) * 1\n",
    "backward_swish = lambda x : (1. - sigmoid(x))*swish(x) + sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_softmax(x):\n",
    "    t=softmax(x)\n",
    "    return np.array([[ t[i][0]*(1.-t[i][0]) if j==i else -t[i][0]*t[j][0] for j in range(t.shape[0]) ] for i in range(t.shape[0]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124a75f",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ebc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To store training and test set losses and accurate predictions\n",
    "# for visualization.\n",
    "store_training_loss = []\n",
    "store_training_accurate_pred = []\n",
    "store_test_loss = []\n",
    "store_test_accurate_pred = []\n",
    "\n",
    "learning_rate = 0.001 # for gradient descent\n",
    "epochs = 100 # number of iteration\n",
    "hidden_size_1 = 392 \n",
    "hidden_size_2 = 98\n",
    "hidden_size_3 = 10 \n",
    "pixels_per_image = 784\n",
    "\n",
    "##We initialize the weights and bias. \n",
    "##Our network is fully connected so one node is linked to all nodes of the next layer\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "generate_weight = lambda layer_1, layer_2 :0.2 * rng.random((layer_2,layer_1 )) - 0.1\n",
    "weights_1 = generate_weight(pixels_per_image, hidden_size_1)\n",
    "weights_2 = generate_weight(hidden_size_1, hidden_size_2)\n",
    "weights_3 = generate_weight(hidden_size_2, hidden_size_3)\n",
    "generate_bias = lambda layer_size :  0.05 * rng.random((layer_size,1))\n",
    "bias_1 = generate_bias(hidden_size_1)\n",
    "bias_2 = generate_bias(hidden_size_2)\n",
    "bias_3 = generate_bias(hidden_size_3)\n",
    "# This is a training loop.\n",
    "# Run the learning experiment for a defined number of epochs (iterations).\n",
    "print(\"start training\")\n",
    "in_layer = lambda ins, weights,bias : np.add(np.dot(weights,ins ),bias)\n",
    "nb_pred=0\n",
    "nb_iter=0\n",
    "for i in range(epochs):\n",
    "    training_loss=0.0\n",
    "    training_accurate_pred = 0.0\n",
    "    for j in range(len(training_images)):\n",
    "        # get the input and output of nodes\n",
    "        labels = training_labels[j].reshape(10,1)\n",
    "        input_img=training_images[i].reshape(pixels_per_image,1)\n",
    "        in_layer_1 = in_layer(input_img,weights_1,bias_1)\n",
    "        out_layer_1 = relu(in_layer_1)\n",
    "        in_layer_2 = in_layer(out_layer_1,weights_2,bias_2)\n",
    "        #out_layer_2 = relu(in_layer_2)\n",
    "        out_layer_2 = swish(in_layer_2)\n",
    "        in_layer_3 = in_layer(out_layer_2,weights_3, bias_3)\n",
    "        out_layer_3 = softmax(in_layer_3)\n",
    "        #out_layer_3= sigmoid(in_layer_3)\n",
    "        #get the loss\n",
    "        training_loss += categorical_cross_entropy_derivative(labels,out_layer_3)\n",
    "        \n",
    "        #compute the derivative for back propagation\n",
    "        # last layer\n",
    "        print(backward_softmax(in_layer_3)[1][:3])\n",
    "        print(categorical_cross_entropy_derivative(out_layer_3, labels)[1][:3])\n",
    "        \"\"\"  issue with this dz3: too big  \"\"\"\n",
    "        dz3 =  np.matmul(backward_softmax(in_layer_3),  categorical_cross_entropy_derivative(out_layer_3, labels))\n",
    "        #dz3 = out_layer_3 - labels #case of sigmoid in last layer\n",
    "        dw3 = dz3 * out_layer_2.T\n",
    "        db3 = dz3.copy()\n",
    "        #2nd layer\n",
    "        dz2 = np.dot(weights_3.T, dz3)*backward_swish(in_layer_2)\n",
    "        #dz2 = np.dot(weights_3.T, dz3)*backward_relu(in_layer_2)\n",
    "        dw2 = dz2 * out_layer_1.T\n",
    "        db2 = dz2.copy()\n",
    "        #first layer\n",
    "        dz1 = np.dot(weights_2.T, dz2)*backward_relu(in_layer_1)\n",
    "        dw1 = dz1 * input_img.T\n",
    "        db1 = dz1.copy()\n",
    "        \n",
    "        #update the weights\n",
    "        #print(\"i = \",i,\", j = \", j,\", dw3[0][0] = \",dw3[0][0],\", weights_3[0][0] = \",weights_3[0][0])\n",
    "        weights_3 -= learning_rate * dw3\n",
    "        #print(\"new weights_3[0][0] = \",weights_3[0][0])\n",
    "        weights_2 -= learning_rate * dw2\n",
    "        weights_1 -= learning_rate * dw1\n",
    "        #update the bias\n",
    "        bias_3 = bias_3 - learning_rate * db3\n",
    "        bias_2 = bias_2 - learning_rate * db2\n",
    "        bias_1 = bias_1 - learning_rate * db1\n",
    "        \"\"\"debugging\n",
    "        print(\"------- out layer 1--------\")\n",
    "        print(out_layer_1[1])\n",
    "        print(\"------- out layer 2--------\")\n",
    "        print(out_layer_2[1])\n",
    "        print(\"------- out layer 3--------\")\n",
    "        print(out_layer_3[1])\n",
    "        print(\"------- weight1--------\")\n",
    "        print(weights_1[1][:5])\n",
    "        print(\"------- weight2--------\")\n",
    "        print(weights_2[1][:5])\n",
    "        print(\"------- weight3--------\")\n",
    "        print(weights_3[1][:5])\n",
    "        print(\"------- dw1--------\")\n",
    "        print(dw1[1][:5])\n",
    "        print(\"------- dw2--------\")\n",
    "        print(dw2[1][:5])\n",
    "        print(\"------- dw3--------\")\n",
    "        print(dw3[1][:5])\n",
    "        break\"\"\"\n",
    "        if j==len(training_images)-1:\n",
    "            print( \"pred \",out_layer_3.T, \", true \",labels.T)\n",
    "        training_accurate_pred += int(np.argmax(out_layer_3) == np.argmax(labels))\n",
    "        nb_iter+=1\n",
    "    # Store training set losses and accurate predictions.\n",
    "    #break\n",
    "    store_training_loss.append(training_loss)\n",
    "    store_training_accurate_pred.append(training_accurate_pred)\n",
    "print(\"training loss\",store_training_loss)\n",
    "print(\"accurate_pred\",store_training_accurate_pred)\n",
    "print(nb_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799bf08",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63743779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results:\n",
    "n_c = 0\n",
    "for i in range(x_test.shape[0]):\n",
    "    x = x_test[i].reshape((x_test[i].size, 1))\n",
    "    y = y_test[i]\n",
    "    \n",
    "    input_img=training_images[i].reshape(pixels_per_image,1)\n",
    "    out_layer_1 = relu(in_layer(input_img,weights_1,bias_1))\n",
    "    #out_layer_2 = relu(in_layer(out_layer_1,weights_2,bias_2))\n",
    "    out_layer_2 = swish(in_layer(out_layer_1,weights_2,bias_2))\n",
    "    out_layer_3 = softmax(in_layer(out_layer_2,weights_3, bias_3))\n",
    "    \n",
    "    n_c+=int(np.argmax(out_layer_3) == np.argmax(y))\n",
    "\n",
    "print(\"Test Accuracy\", (n_c/X_test.shape[0])*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
